{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting data preprocessing pipeline...\n",
            "PyTorch version: 2.0.1\n",
            "CUDA available: True\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import pickle\n",
        "import enum\n",
        "import os\n",
        "os.makedirs('./processed_dataset', exist_ok=True)\n",
        "\n",
        "print(\"Starting data preprocessing pipeline...\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Utility functions defined successfully.\n"
          ]
        }
      ],
      "source": [
        "# Define utility functions and enums\n",
        "class NodeType(enum.IntEnum):\n",
        "    \"\"\"Node type enumeration for boundary conditions.\"\"\"\n",
        "    NORMAL = 0\n",
        "    WALL_BOUNDARY = 1\n",
        "    SIZE = 2\n",
        "\n",
        "def triangles_to_edges(faces):\n",
        "    \"\"\"\n",
        "    Convert triangle faces to edge connectivity.\n",
        "    \n",
        "    Args:\n",
        "        faces (torch.Tensor): Triangle faces [F, 3]\n",
        "        \n",
        "    Returns:\n",
        "        tuple: (two_way_connectivity, min_to_max_connectivity)\n",
        "    \"\"\"\n",
        "    edges = torch.cat((\n",
        "        faces[:, 0:2],\n",
        "        faces[:, 1:3],\n",
        "        torch.stack((faces[:, 2], faces[:, 0]), dim=1)\n",
        "    ), dim=0)\n",
        "    \n",
        "    receivers, _ = torch.min(edges, dim=1)\n",
        "    senders, _ = torch.max(edges, dim=1)\n",
        "    packed_edges = torch.stack((senders, receivers), dim=1)\n",
        "    unique_edges = torch.unique(packed_edges, return_inverse=False, return_counts=False, dim=0)\n",
        "    senders, receivers = torch.unbind(unique_edges, dim=1)\n",
        "    senders = senders.to(torch.int64)\n",
        "    receivers = receivers.to(torch.int64)\n",
        "\n",
        "    min_to_max_connectivity = torch.stack((senders, receivers), dim=0)\n",
        "    two_way_connectivity = torch.stack((\n",
        "        torch.cat((senders, receivers), dim=0), \n",
        "        torch.cat((receivers, senders), dim=0)\n",
        "    ))\n",
        "    \n",
        "    return two_way_connectivity, min_to_max_connectivity\n",
        "\n",
        "def preprocess_node_types(data_list):\n",
        "    \"\"\"\n",
        "    Preprocess node types by converting boundary node codes to standard format.\n",
        "    \n",
        "    Args:\n",
        "        data_list (list): List of trajectory data\n",
        "    \n",
        "    Returns:\n",
        "        list: Processed data with standardized node types\n",
        "    \"\"\"\n",
        "    processed_data = []\n",
        "    \n",
        "    for trace in data_list:\n",
        "        processed_trace = []\n",
        "        for time_step in trace:\n",
        "            processed_step = time_step.copy()\n",
        "            \n",
        "            # Convert node type 6 to WALL_BOUNDARY\n",
        "            node_types = processed_step['node_type'].copy()\n",
        "            for i, node_type in enumerate(node_types):\n",
        "                if node_type == 6:\n",
        "                    node_types[i] = NodeType.WALL_BOUNDARY\n",
        "            \n",
        "            processed_step['node_type'] = node_types\n",
        "            processed_trace.append(processed_step)\n",
        "        \n",
        "        processed_data.append(processed_trace)\n",
        "    \n",
        "    return processed_data\n",
        "\n",
        "print(\"Utility functions defined successfully.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading raw data...\n",
            "Loaded training data: 30 trajectories\n",
            "Loaded test data: 6 trajectories\n",
            "Sample data keys: dict_keys(['cells', 'mesh_pos', 'node_type', 'world_pos', 'prev|world_pos', 'target|world_pos', 'velocity', 'dt', 'youngs'])\n",
            "Test data keys: dict_keys(['cells', 'mesh_pos', 'node_type', 'world_pos', 'prev|world_pos', 'target|world_pos', 'velocity', 'dt', 'youngs'])\n",
            "Selected 30 training trajectories\n",
            "Selected 6 test trajectories\n",
            "Node type preprocessing completed.\n"
          ]
        }
      ],
      "source": [
        "# Load raw data\n",
        "print(\"Loading raw data...\")\n",
        "\n",
        "# Define data paths\n",
        "dataset_path = \"./raw_dataset/train.pickle\"\n",
        "test_path = \"./raw_dataset/test.pickle\"\n",
        "processed_dir = \"./processed_dataset/\"\n",
        "\n",
        "# Load training and test data\n",
        "with open(dataset_path, 'rb') as file:\n",
        "    data_train = pickle.load(file)\n",
        "    print(f\"Loaded training data: {len(data_train)} trajectories\")\n",
        "\n",
        "with open(test_path, 'rb') as file:\n",
        "    data_test = pickle.load(file)\n",
        "    print(f\"Loaded test data: {len(data_test)} trajectories\")\n",
        "\n",
        "# Show data structure\n",
        "print(f\"Sample data keys: {data_train[0][0].keys()}\")\n",
        "print(f\"Test data keys: {data_test[0][0].keys()}\")\n",
        "\n",
        "# # Select subsets for processing\n",
        "# data_train = data_train[0:10]\n",
        "# data_test = data_test[4:6]\n",
        "\n",
        "print(f\"Selected {len(data_train)} training trajectories\")\n",
        "print(f\"Selected {len(data_test)} test trajectories\")\n",
        "\n",
        "# Preprocess node types\n",
        "data_train = preprocess_node_types(data_train)\n",
        "data_test = preprocess_node_types(data_test)\n",
        "\n",
        "print(\"Node type preprocessing completed.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data preparation function defined.\n"
          ]
        }
      ],
      "source": [
        "def preparation(save_name, data):\n",
        "    \"\"\"\n",
        "    Prepare raw trajectory data for training by converting to graph format.\n",
        "    \n",
        "    Args:\n",
        "        save_name (str): Name for saving processed data\n",
        "        data (list): List of trajectory data\n",
        "        \n",
        "    Returns:\n",
        "        None: Saves processed data to file\n",
        "    \"\"\"\n",
        "    number_trajectories = len(data)\n",
        "    data_to_graph = []\n",
        "    \n",
        "    for i, trajectory in enumerate(data):\n",
        "        if i == number_trajectories:\n",
        "            break\n",
        "            \n",
        "        print(f\"Processing trajectory: {i}\")\n",
        "        tra = {\n",
        "            \"edge_index\": [],\n",
        "            \"edge_attr\": [],\n",
        "            \"mesh_edge_attr\": [],\n",
        "            \"cells\": [],\n",
        "            \"world_pos\": [],\n",
        "            \"prev_world_pos\": [],\n",
        "            \"target_world_pos\": [],\n",
        "            \"velocity\": [],\n",
        "            \"youngs\": [],\n",
        "            \"one_hot_node_type\": [],\n",
        "            \"node_type\": [],\n",
        "        }\n",
        "        data_to_graph.append(tra)\n",
        "        number_ts = len(trajectory)\n",
        "\n",
        "        for ts in range(number_ts):\n",
        "            if ts == 400:  # Limit time steps\n",
        "                break\n",
        "                \n",
        "            inputs = trajectory[ts]\n",
        "\n",
        "            # Extract and convert data\n",
        "            world_pos = torch.squeeze(torch.tensor(inputs['world_pos']))\n",
        "            prev_world_pos = torch.squeeze(torch.tensor(inputs['prev|world_pos']))\n",
        "            target_world_pos = torch.squeeze(torch.tensor(inputs['target|world_pos']))\n",
        "            cells = torch.squeeze(torch.tensor(inputs['cells']))\n",
        "            youngs = torch.squeeze(torch.tensor(inputs['youngs']))\n",
        "            mesh_pos = torch.squeeze(torch.tensor(inputs['mesh_pos']))\n",
        "            node_type = torch.tensor(inputs['node_type'])\n",
        "\n",
        "            # Calculate velocity\n",
        "            velocity = (world_pos - prev_world_pos) / 0.1\n",
        "\n",
        "            # Create one-hot encoding for node types\n",
        "            one_hot_node_type = F.one_hot(node_type[:, 0].to(torch.int64), NodeType.SIZE)\n",
        "\n",
        "            # Prepare material properties\n",
        "            node_number = world_pos.shape[0]\n",
        "            youngs = youngs.reshape(1, 1).repeat(node_number, 1) / 1e6\n",
        "\n",
        "            # Generate edge connectivity\n",
        "            edge_index, _ = triangles_to_edges(cells)\n",
        "\n",
        "            # Calculate edge features\n",
        "            m_i = mesh_pos[edge_index[0]]\n",
        "            m_j = mesh_pos[edge_index[1]]\n",
        "            u_i = world_pos[edge_index[0]]\n",
        "            u_j = world_pos[edge_index[1]]\n",
        "\n",
        "            m_ij = m_i - m_j\n",
        "            u_ij = u_i - u_j\n",
        "\n",
        "            m_ij_norm = torch.norm(m_ij, p=2, dim=1, keepdim=True)\n",
        "            u_ij_norm = torch.norm(u_ij, p=2, dim=1, keepdim=True)\n",
        "\n",
        "            edge_attr = torch.cat((u_ij, u_ij_norm), dim=-1).float()\n",
        "            mesh_edge_attr = torch.cat((m_ij, m_ij_norm), dim=-1).float()\n",
        "\n",
        "            # Convert to appropriate types\n",
        "            youngs = youngs.float()\n",
        "            one_hot_node_type = one_hot_node_type.float()\n",
        "            node_type = node_type.float()\n",
        "            velocity = velocity.float()\n",
        "            edge_index = edge_index.long()\n",
        "\n",
        "            # Store processed data\n",
        "            data_to_graph[i][\"edge_index\"].append(edge_index)\n",
        "            data_to_graph[i][\"edge_attr\"].append(edge_attr)\n",
        "            data_to_graph[i][\"mesh_edge_attr\"].append(mesh_edge_attr)\n",
        "            data_to_graph[i][\"cells\"].append(cells)\n",
        "            data_to_graph[i][\"world_pos\"].append(world_pos)\n",
        "            data_to_graph[i][\"prev_world_pos\"].append(prev_world_pos)\n",
        "            data_to_graph[i][\"target_world_pos\"].append(target_world_pos)\n",
        "            data_to_graph[i][\"velocity\"].append(velocity)\n",
        "            data_to_graph[i][\"youngs\"].append(youngs)\n",
        "            data_to_graph[i][\"one_hot_node_type\"].append(one_hot_node_type)\n",
        "            data_to_graph[i][\"node_type\"].append(node_type)\n",
        "\n",
        "        # Stack temporal dimension\n",
        "        for key in data_to_graph[i].keys():\n",
        "            data_to_graph[i][key] = torch.stack(data_to_graph[i][key], dim=0)\n",
        "\n",
        "    print(f\"Sample processed data keys: {data_to_graph[0].keys()}\")\n",
        "    # torch.save(data_to_graph, f'./{save_name}.pt')\n",
        "    torch.save(data_to_graph, os.path.join(processed_dir, f'{save_name}.pt'))\n",
        "    print(f\"Saved processed data to {save_name}.pt\")\n",
        "\n",
        "print(\"Data preparation function defined.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Statistics calculation function defined.\n"
          ]
        }
      ],
      "source": [
        "def normalize(to_normalize, mean_vec, std_vec):\n",
        "    \"\"\"Normalize tensor using mean and standard deviation.\"\"\"\n",
        "    return (to_normalize - mean_vec) / std_vec\n",
        "\n",
        "def unnormalize(to_unnormalize, mean_vec, std_vec):\n",
        "    \"\"\"Unnormalize tensor using mean and standard deviation.\"\"\"\n",
        "    return to_unnormalize * std_vec + mean_vec\n",
        "\n",
        "def get_stats(data_list, save_name):\n",
        "    \"\"\"\n",
        "    Calculate normalization statistics for the dataset.\n",
        "    \n",
        "    Args:\n",
        "        data_list (list): List of processed trajectory data\n",
        "        save_name (str): Name for saving statistics\n",
        "        \n",
        "    Returns:\n",
        "        None: Saves statistics to file\n",
        "    \"\"\"\n",
        "    mean_std = {\n",
        "        \"vel_mean\": [],\n",
        "        \"vel_std\": [],\n",
        "        \"youngs_mean\": [],\n",
        "        \"youngs_std\": [],\n",
        "        \"edge_mean\": [],\n",
        "        \"edge_std\": [],\n",
        "        \"mesh_edge_mean\": [],\n",
        "        \"mesh_edge_std\": [],\n",
        "        \"node_size\": [],\n",
        "        \"edge_size\": [],\n",
        "    }\n",
        "    eps = 1e-8\n",
        "\n",
        "    # Collect all data across trajectories and time steps\n",
        "    all_vel = torch.cat([traj[\"velocity\"].reshape(-1, 3) for traj in data_list], dim=0)\n",
        "    all_youngs = torch.cat([traj[\"youngs\"].reshape(-1, traj[\"youngs\"].shape[-1]) for traj in data_list], dim=0)\n",
        "    all_edge = torch.cat([traj[\"edge_attr\"].reshape(-1, traj[\"edge_attr\"].shape[-1]) for traj in data_list], dim=0)\n",
        "    all_mesh_edge = torch.cat([traj[\"mesh_edge_attr\"].reshape(-1, traj[\"mesh_edge_attr\"].shape[-1]) for traj in data_list], dim=0)\n",
        "\n",
        "    # Calculate sizes\n",
        "    edge_size = all_edge.size(1) + all_mesh_edge.size(1)\n",
        "    node_size = all_youngs.size(1) + data_list[0][\"one_hot_node_type\"][0].size(1)\n",
        "    \n",
        "    print(f\"Node feature size: {node_size}\")\n",
        "    print(f\"Edge feature size: {edge_size}\")\n",
        "    \n",
        "    # Calculate statistics\n",
        "    mean_std[\"vel_mean\"] = all_vel.mean(dim=0)\n",
        "    mean_std[\"vel_std\"] = torch.sqrt(all_vel.var(dim=0))\n",
        "    mean_std[\"youngs_mean\"] = all_youngs.mean(dim=0)\n",
        "    mean_std[\"youngs_std\"] = torch.sqrt(all_youngs.var(dim=0))\n",
        "    mean_std[\"edge_mean\"] = all_edge.mean(dim=0)\n",
        "    mean_std[\"edge_std\"] = torch.sqrt(all_edge.var(dim=0))\n",
        "    mean_std[\"mesh_edge_mean\"] = all_mesh_edge.mean(dim=0)\n",
        "    mean_std[\"mesh_edge_std\"] = torch.sqrt(all_mesh_edge.var(dim=0))\n",
        "\n",
        "    mean_std[\"node_size\"] = torch.tensor([node_size])\n",
        "    mean_std[\"edge_size\"] = torch.tensor([edge_size])\n",
        "\n",
        "    # Clamp standard deviations to avoid division by zero\n",
        "    for key in mean_std:\n",
        "        if \"std\" in key:\n",
        "            mean_std[key] = torch.clamp(mean_std[key], min=eps)\n",
        "\n",
        "    # torch.save(mean_std, f'./{save_name}.pt')\n",
        "    torch.save(mean_std, os.path.join(processed_dir, f'{save_name}.pt'))\n",
        "    print(f\"Saved statistics to {save_name}.pt\")\n",
        "\n",
        "print(\"Statistics calculation function defined.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "EXECUTING DATA PREPROCESSING PIPELINE\n",
            "==================================================\n",
            "\n",
            "Processing training data...\n",
            "Processing trajectory: 0\n",
            "Processing trajectory: 1\n",
            "Processing trajectory: 2\n",
            "Processing trajectory: 3\n",
            "Processing trajectory: 4\n",
            "Processing trajectory: 5\n",
            "Processing trajectory: 6\n",
            "Processing trajectory: 7\n",
            "Processing trajectory: 8\n",
            "Processing trajectory: 9\n",
            "Processing trajectory: 10\n",
            "Processing trajectory: 11\n",
            "Processing trajectory: 12\n",
            "Processing trajectory: 13\n",
            "Processing trajectory: 14\n",
            "Processing trajectory: 15\n",
            "Processing trajectory: 16\n",
            "Processing trajectory: 17\n",
            "Processing trajectory: 18\n",
            "Processing trajectory: 19\n",
            "Processing trajectory: 20\n",
            "Processing trajectory: 21\n",
            "Processing trajectory: 22\n",
            "Processing trajectory: 23\n",
            "Processing trajectory: 24\n",
            "Processing trajectory: 25\n",
            "Processing trajectory: 26\n",
            "Processing trajectory: 27\n",
            "Processing trajectory: 28\n",
            "Processing trajectory: 29\n",
            "Sample processed data keys: dict_keys(['edge_index', 'edge_attr', 'mesh_edge_attr', 'cells', 'world_pos', 'prev_world_pos', 'target_world_pos', 'velocity', 'youngs', 'one_hot_node_type', 'node_type'])\n",
            "Saved processed data to train.pt\n",
            "\n",
            "Processing test data...\n",
            "Processing trajectory: 0\n",
            "Processing trajectory: 1\n",
            "Processing trajectory: 2\n",
            "Processing trajectory: 3\n",
            "Processing trajectory: 4\n",
            "Processing trajectory: 5\n",
            "Sample processed data keys: dict_keys(['edge_index', 'edge_attr', 'mesh_edge_attr', 'cells', 'world_pos', 'prev_world_pos', 'target_world_pos', 'velocity', 'youngs', 'one_hot_node_type', 'node_type'])\n",
            "Saved processed data to test.pt\n",
            "\n",
            "Loading processed data for statistics calculation...\n",
            "\n",
            "Calculating normalization statistics...\n",
            "Node feature size: 3\n",
            "Edge feature size: 8\n",
            "Saved statistics to stats_train.pt\n",
            "\n",
            "==================================================\n",
            "DATA PREPROCESSING COMPLETED SUCCESSFULLY\n",
            "==================================================\n",
            "Generated files:\n",
            "- train.pt: Processed training data\n",
            "- test.pt: Processed test data\n",
            "- stats_train.pt: Normalization statistics\n",
            "\n",
            "Data is ready for training!\n"
          ]
        }
      ],
      "source": [
        "# Execute data preprocessing pipeline\n",
        "print(\"=\" * 50)\n",
        "print(\"EXECUTING DATA PREPROCESSING PIPELINE\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Process training data\n",
        "print(\"\\nProcessing training data...\")\n",
        "preparation('train', data_train)\n",
        "\n",
        "# Process test data\n",
        "print(\"\\nProcessing test data...\")\n",
        "preparation('test', data_test)\n",
        "\n",
        "# Load processed data for statistics calculation\n",
        "print(\"\\nLoading processed data for statistics calculation...\")\n",
        "# dataset_train = torch.load('train.pt')\n",
        "# dataset_test = torch.load('test.pt')\n",
        "dataset_train = torch.load(os.path.join(processed_dir, 'train.pt'))\n",
        "dataset_test = torch.load(os.path.join(processed_dir, 'test.pt'))\n",
        "\n",
        "# Calculate and save statistics\n",
        "print(\"\\nCalculating normalization statistics...\")\n",
        "get_stats(dataset_train, \"stats_train\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"DATA PREPROCESSING COMPLETED SUCCESSFULLY\")\n",
        "print(\"=\" * 50)\n",
        "print(\"Generated files:\")\n",
        "print(\"- train.pt: Processed training data\")\n",
        "print(\"- test.pt: Processed test data\") \n",
        "print(\"- stats_train.pt: Normalization statistics\")\n",
        "print(\"\\nData is ready for training!\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "gnn3.8",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
